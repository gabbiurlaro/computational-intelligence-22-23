{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate\n",
    "from operator import xor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy, copy\n",
    "from nim_utils import *\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, nim, alpha = 0.1, random_factor = 0.2) -> None:\n",
    "        self.state_history = []\n",
    "        self.alpha = alpha\n",
    "        self.random_factor = random_factor\n",
    "        # Q table, initially empty\n",
    "        self.Q = {}\n",
    "        self.init_q(nim)\n",
    "\n",
    "    def init_q(self, nim) -> dict:\n",
    "        result = []\n",
    "        def generate_states(rows, current_state):\n",
    "            if len(current_state) == len(rows):\n",
    "                # We have generated a valid state \n",
    "                result.append(current_state)\n",
    "                return\n",
    "            # Generate all possible states for the current row\n",
    "            for i in range(rows[len(current_state)] + 1):\n",
    "                new_state = current_state + [i]\n",
    "                generate_states(rows, new_state)\n",
    "        \n",
    "        generate_states(nim.rows, [])\n",
    "        for state in result:\n",
    "            x = Nim(len(nim.rows), state=state)\n",
    "            for action in x.possible_moves():\n",
    "                self.Q[(x, action)] = np.random.uniform(low=1.0, high=0.1)     \n",
    "\n",
    "    def choose_action(self, state) -> Nimply:\n",
    "        maxG = -10e15\n",
    "        next_move = None\n",
    "        possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1)]\n",
    "        if np.random.random() < self.random_factor:\n",
    "            next_move = random.choice(possible_moves) \n",
    "        else:\n",
    "            for action in possible_moves:\n",
    "                # print(f\"Action: {action}\")\n",
    "                new_state = deepcopy(state)\n",
    "                new_state.nimming(action)\n",
    "                # print(f\"Check {new_state}\")\n",
    "                if self.G[new_state] >= maxG:\n",
    "                    next_move = action\n",
    "                    maxG = self.G[new_state]\n",
    "        return next_move\n",
    "\n",
    "    def update_state_history(self, state, reward):\n",
    "        self.state_history.append((state, reward))\n",
    "\n",
    "    def learn(self):\n",
    "        target = 0\n",
    "        for prev, reward in reversed(self.state_history):\n",
    "            self.G[prev] = (1 - self.alpha)*self.G[prev] + self.alpha * (target - self.G[prev])\n",
    "            target += reward\n",
    "        self.state_history = []\n",
    "        self.random_factor -= 10e-5  # decrease random factor each episode of play\n",
    "\n",
    "    def get_strategy(self) -> Callable:\n",
    "        def agent_strategy(state: Nim) -> Nimply:\n",
    "            action = self.choose_action(state)\n",
    "            # print(f\"Choosen action: {action}\")\n",
    "            return Nimply(action[0], action[1])\n",
    "        return agent_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_against(strategy: Callable, against: Callable, NIM_SIZE: int, NUM_MATCHES = 100) -> float:\n",
    "    opponent = (strategy, against)\n",
    "    won = 0\n",
    "    for _ in range(NUM_MATCHES):\n",
    "        nim = Nim(NIM_SIZE)\n",
    "        player = 0\n",
    "        while nim:\n",
    "            # logging.debug(nim)\n",
    "            ply = opponent[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1: # winner is the zero\n",
    "            won += 1\n",
    "        # logging.debug(f\"player {1 - player} has won.\")\n",
    "    return won / NUM_MATCHES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size = 2\n"
     ]
    }
   ],
   "source": [
    "# Costants\n",
    "NUM_EPOCHS = 100000\n",
    "NIM_SIZE_learn = 2\n",
    "OPPONENTS = [aggressive] * 5 + [optimal_strategy]\n",
    "\n",
    "# Create the agent\n",
    "game = Nim(NIM_SIZE_learn)\n",
    "print(f\"size = {len(game.rows)}\")\n",
    "agent = Agent(game, alpha=0.99, random_factor=.1)\n",
    "\n",
    "states = agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<0 1>, (1, 1): 0.5325767768100627\n",
      "<0 2>, (1, 1): 0.6515700550948612\n",
      "<0 2>, (1, 2): 0.17030622417935637\n",
      "<0 3>, (1, 1): 0.5428793574177256\n",
      "<0 3>, (1, 2): 0.92818945473787\n",
      "<0 3>, (1, 3): 0.9710447536309582\n",
      "<1 0>, (0, 1): 0.7805588776908537\n",
      "<1 1>, (0, 1): 0.6069205158619628\n",
      "<1 1>, (1, 1): 0.2701561610913368\n",
      "<1 2>, (0, 1): 0.565036555603934\n",
      "<1 2>, (1, 1): 0.4197333442194956\n",
      "<1 2>, (1, 2): 0.207675511445776\n",
      "<1 3>, (0, 1): 0.48511230833451613\n",
      "<1 3>, (1, 1): 0.5269384803095291\n",
      "<1 3>, (1, 2): 0.22688082846673074\n",
      "<1 3>, (1, 3): 0.45216064351913143\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    print(f\"{state[0]}, {state[1]}: {states[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'G'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m current_game:\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m turn:\n\u001b[0;32m     12\u001b[0m         \u001b[39m# my agent turn\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m         action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mchoose_action(current_game)\n\u001b[0;32m     14\u001b[0m         \u001b[39m# print(f\"Action: {action}\")\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         current_game\u001b[39m.\u001b[39mnimming(action)\n",
      "Cell \u001b[1;32mIn [10], line 40\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     38\u001b[0m new_state\u001b[39m.\u001b[39mnimming(action)\n\u001b[0;32m     39\u001b[0m \u001b[39m# print(f\"Check {new_state}\")\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mG[new_state] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m maxG:\n\u001b[0;32m     41\u001b[0m     next_move \u001b[39m=\u001b[39m action\n\u001b[0;32m     42\u001b[0m     maxG \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG[new_state]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'G'"
     ]
    }
   ],
   "source": [
    "# for state in states:\n",
    "#     print(f\"{state}: {states[state]}\")\n",
    "win_logs = []\n",
    "scores = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # logging.debug(f\"Starting the game #{i}\")\n",
    "    current_game = Nim(NIM_SIZE_learn)\n",
    "    turn = True\n",
    "    OPPONENT = random.choice(OPPONENTS)\n",
    "    while current_game:\n",
    "        if turn:\n",
    "            # my agent turn\n",
    "            action = agent.choose_action(current_game)\n",
    "            # print(f\"Action: {action}\")\n",
    "            current_game.nimming(action)\n",
    "            state, reward = current_game.get_state_and_reward()\n",
    "            # print(f\"{state} -> {reward}\")\n",
    "            agent.update_state_history(state, reward)\n",
    "        else:\n",
    "            action = OPPONENT(current_game)\n",
    "            current_game.nimming(action)\n",
    "        turn = not turn\n",
    "    winner = int(not turn)\n",
    "    win_logs.append(winner)\n",
    "    # logging.debug(f\"The game ended. Player {winner} wins\")\n",
    "    agent.learn()\n",
    "    if i % 300 == 0:\n",
    "        score = evaluate_against(agent.get_strategy(), aggressive)\n",
    "        scores.append(score)\n",
    "        print(f\"#{i}: {score}\")\n",
    "\n",
    "\n",
    "agent_strategy = agent.get_strategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(win_logs)), win_logs, marker='o', linewidths=.2, edgecolors=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(scores)), scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_against(agent_strategy, aggressive)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('ci22-8VIwGAvU-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c0b5ebbcc54db0eea4215a72b49a4750117c977bb82fac66e9d6a0397f1ffaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
