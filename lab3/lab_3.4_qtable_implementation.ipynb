{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate\n",
    "from operator import xor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy, copy\n",
    "from nim_utils import *\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, nim, alpha = 0.1, random_factor = 0.2) -> None:\n",
    "        self.state_history = []\n",
    "        self.alpha = alpha\n",
    "        self.random_factor = random_factor\n",
    "        # Q table, initially empty\n",
    "        self.Q = {}\n",
    "        self.init_q(nim)\n",
    "\n",
    "    def init_q(self, nim) -> dict:\n",
    "        result = []\n",
    "        def generate_states(rows, current_state):\n",
    "            if len(current_state) == len(rows):\n",
    "                # We have generated a valid state \n",
    "                result.append(current_state)\n",
    "                return\n",
    "            # Generate all possible states for the current row\n",
    "            for i in range(rows[len(current_state)] + 1):\n",
    "                new_state = current_state + [i]\n",
    "                generate_states(rows, new_state)\n",
    "        \n",
    "        generate_states(nim.rows, [])\n",
    "        for state in result:\n",
    "            x = Nim(len(nim.rows), state=state)\n",
    "            for action in x.possible_moves():\n",
    "                self.Q[(x, action)] = np.random.uniform(low=1.0, high=0.1)     \n",
    "\n",
    "    def choose_action(self, state) -> Nimply:\n",
    "        maxG = -10e15\n",
    "        next_move = None\n",
    "        possible_moves = state.possible_moves()\n",
    "        if np.random.random() < self.random_factor:\n",
    "            next_move = random.choice(possible_moves) \n",
    "        else:\n",
    "            for action in possible_moves:\n",
    "                # print(f\"Check {new_state}\")\n",
    "                if self.Q[(state, action)] >= maxG:\n",
    "                    next_move = action\n",
    "                    maxG = self.Q[(state, action)]\n",
    "        return next_move\n",
    "\n",
    "    def update_state_history(self, state, action,  reward):\n",
    "        self.state_history.append(((state, action), reward))\n",
    "\n",
    "    def learn(self):\n",
    "        target = 0\n",
    "        for prev, reward in reversed(self.state_history):\n",
    "            state, action = prev\n",
    "            print(f\"{state}{action}\")\n",
    "            self.Q[prev] = self.Q[prev] + self.alpha * (target - self.Q[prev])\n",
    "            target += reward\n",
    "        self.state_history = []\n",
    "        self.random_factor -= 10e-5  # decrease random factor each episode of play\n",
    "\n",
    "    def get_strategy(self) -> Callable:\n",
    "        def agent_strategy(state: Nim) -> Nimply:\n",
    "            action = self.choose_action(state)\n",
    "            # print(f\"Choosen action: {action}\")\n",
    "            return Nimply(action[0], action[1])\n",
    "        return agent_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_against(strategy: Callable, against: Callable, NIM_SIZE: int, NUM_MATCHES = 100) -> float:\n",
    "    opponent = (strategy, against)\n",
    "    won = 0\n",
    "    for _ in range(NUM_MATCHES):\n",
    "        nim = Nim(NIM_SIZE)\n",
    "        player = 0\n",
    "        while nim:\n",
    "            # logging.debug(nim)\n",
    "            ply = opponent[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1: # winner is the zero\n",
    "            won += 1\n",
    "        # logging.debug(f\"player {1 - player} has won.\")\n",
    "    return won / NUM_MATCHES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size = 2\n"
     ]
    }
   ],
   "source": [
    "# Costants\n",
    "NUM_EPOCHS = 5000\n",
    "NIM_SIZE_learn = 2\n",
    "OPPONENTS = [aggressive]\n",
    "\n",
    "# Create the agent\n",
    "game = Nim(NIM_SIZE_learn)\n",
    "print(f\"size = {len(game.rows)}\")\n",
    "agent = Agent(game, alpha=0.1, random_factor=.1)\n",
    "\n",
    "states = agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for state in states:\n",
    "#     print(f\"{state[0]}, {state[1]}: {states[state]}\")\n",
    "\n",
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<0 0>(1, 1)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(<nim_utils.Nim object at 0x000001445CDBC130>, (1, 1))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [80], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m win_logs\u001b[39m.\u001b[39mappend(winner)\n\u001b[0;32m     26\u001b[0m \u001b[39m# logging.debug(f\"The game ended. Player {winner} wins\")\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m300\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m     score \u001b[39m=\u001b[39m evaluate_against(agent\u001b[39m.\u001b[39mget_strategy(), aggressive, NIM_SIZE\u001b[39m=\u001b[39mNIM_SIZE_learn)\n",
      "Cell \u001b[1;32mIn [76], line 50\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m     state, action \u001b[39m=\u001b[39m prev\n\u001b[0;32m     49\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mstate\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00maction\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ[prev] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha)\u001b[39m*\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQ[prev] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m (target \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ[prev])\n\u001b[0;32m     51\u001b[0m     target \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_history \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyError\u001b[0m: (<nim_utils.Nim object at 0x000001445CDBC130>, (1, 1))"
     ]
    }
   ],
   "source": [
    "# for state in states:\n",
    "#     print(f\"{state}: {states[state]}\")\n",
    "win_logs = []\n",
    "scores = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # logging.debug(f\"Starting the game #{i}\")\n",
    "    current_game = Nim(NIM_SIZE_learn)\n",
    "    turn = True\n",
    "    OPPONENT = random.choice(OPPONENTS)\n",
    "    while current_game:\n",
    "        if turn:\n",
    "            state, _ = current_game.get_state_and_reward()\n",
    "            # my agent turn\n",
    "            action = agent.choose_action(current_game)\n",
    "            # print(f\"Action: {action}\")\n",
    "            current_game.nimming(action)\n",
    "            _, reward = current_game.get_state_and_reward()\n",
    "            # print(f\"{state} -> {reward}\")\n",
    "            agent.update_state_history(state, action, reward)\n",
    "        else:\n",
    "            action = OPPONENT(current_game)\n",
    "            current_game.nimming(action)\n",
    "        turn = not turn\n",
    "    winner = int(not turn)\n",
    "    win_logs.append(winner)\n",
    "    # logging.debug(f\"The game ended. Player {winner} wins\")\n",
    "    agent.learn()\n",
    "    if i % 300 == 0:\n",
    "        score = evaluate_against(agent.get_strategy(), aggressive, NIM_SIZE=NIM_SIZE_learn)\n",
    "        scores.append(score)\n",
    "        print(f\"#{i}: {score}\")\n",
    "\n",
    "\n",
    "agent_strategy = agent.get_strategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(win_logs)), win_logs, marker='o', linewidths=.2, edgecolors=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(scores)), scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_against(agent_strategy, aggressive)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('ci22-8VIwGAvU-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c0b5ebbcc54db0eea4215a72b49a4750117c977bb82fac66e9d6a0397f1ffaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
